{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Offline RL (advantage-weighted SFT) for Qwen-0.5B on RL rewards\n",
        "This notebook fine-tunes your existing Qwen-0.5B SFT model with the reward-annotated dataset `rl_games_positions.jsonl` produced by `train_scripts/rl_pgn_data_prep.py`.\n",
        "It uses a lightweight advantage-weighted SFT style (single pass, top-K by reward) to fit within a Kaggle T4 24h budget."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -U \"transformers>=4.44\" \"trl>=0.10\" peft datasets accelerate huggingface_hub hf-transfer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, os, json\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from peft import LoraConfig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paths and config\n",
        "- `data_path`: reward-annotated JSONL from `train_scripts/rl_pgn_data_prep.py`\n",
        "- `base_model`: your SFT/merged checkpoint (adjust to your HF repo or local path).\n",
        "- `output_dir`: where the RL LoRA adapter will be saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = \"rl_games_positions.jsonl\"  # adjust if stored elsewhere\n",
        "base_model = \"bot-rakshit/qwen-chess-0.5b-108k-merged-new\"  # or local path\n",
        "output_dir = \"qwen-chess-0.5b-rl-aw-sft\"\n",
        "top_k = 50000  # take top-K by reward for speed; set to None to use all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "ds = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
        "if top_k is not None:\n",
        "    ds = ds.sort(\"reward\", reverse=True).select(range(min(top_k, len(ds))))\n",
        "print(ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "tok = os.environ.get('HF_TOKEN')\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, token=tok)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.model_max_length = 384\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    token=tok,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")\n",
        "training_args = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-5,\n",
        "    logging_steps=20,\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    report_to=\"none\",\n",
        "    warmup_ratio=0.03,\n",
        "    gradient_checkpointing=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=ds,\n",
        "    args=training_args,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir)\n",
        "print('Saved adapter to', output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: merge LoRA into full weights\n",
        "Run if you want a single merged checkpoint for inference/submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merge_out = \"qwen-chess-0.5b-rl-aw-merged\"\n",
        "from peft import PeftModel\n",
        "tok_env = os.environ.get('HF_TOKEN')\n",
        "tok = tokenizer  # reuse\n",
        "base = base_model\n",
        "adapter = output_dir\n",
        "model_base = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch.bfloat16, device_map=\"auto\", token=tok_env)\n",
        "model_base = PeftModel.from_pretrained(model_base, adapter)\n",
        "merged = model_base.merge_and_unload()\n",
        "tok.save_pretrained(merge_out)\n",
        "merged.save_pretrained(merge_out)\n",
        "print('Merged model saved to', merge_out)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}